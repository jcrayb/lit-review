{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import ollama\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "keywords = {'optional': ['Energy storage', 'Multistage', 'SDDP', 'rolling horizon'],\n",
    "            'required': ['disaster']}\n",
    "description = \"Paper using SDDP to solve a multi-stage problem to increase resilience of a power grid against hurricanes. The model places mobile energy sources, and once placed minimizes load shed, all the while hedging against forecast uncertainty. Uses a shrinking horizon to account for forecast updates.\"\n",
    "min_length = 3\n",
    "min_year = 2022\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['name', 'year', 'authors', 'journal', 'categories', 'key ideas', 'link'])\n",
    "url = \"https://scholar.google.com/\"\n",
    "\n",
    "combinations = []\n",
    "optional = keywords['optional']\n",
    "for L in range(len(optional) + 1):\n",
    "    for subset in itertools.combinations(optional, L):\n",
    "        if len(subset) + len(keywords['required']) >= min_length:\n",
    "            combinations.append(tuple(keywords['required'] + [s for s in subset]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for c in combinations:#\n",
    "    op = webdriver.ChromeOptions()\n",
    "    #op.add_argument('headless')\n",
    "    driver = webdriver.Chrome(options=op)\n",
    "    query = ', '.join(c)\n",
    "    with driver as browser:\n",
    "        browser.set_window_size(1024, 768)\n",
    "        browser.get(url)\n",
    "        action = ActionChains(driver)\n",
    "\n",
    "        results_div = driver.find_element(By.CSS_SELECTOR, \"input[name='q']\")\n",
    "        action.move_to_element(results_div).click().send_keys(query).perform()\n",
    "        action.move_to_element(results_div).click().send_keys(Keys.ENTER).perform()\n",
    "\n",
    "        #q_button = driver.find_element(By.CSS_SELECTOR, \"#gs_hdr_tsb\")\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "        articles = soup.find_all(\"div\", {\"class\": 'gs_r'})\n",
    "    \n",
    "    new_df = pd.DataFrame(columns=['name', 'year', 'authors', 'journal', 'categories', 'key ideas', 'link'], index=[i for i in range(len(articles)-1)])\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        try:\n",
    "            title_data =  article.find('div', {'class': 'gs_ri'}).find('h3').find('a')\n",
    "            title = title_data.text\n",
    "            link = title_data['href']\n",
    "\n",
    "            author_data = article.find('div', {'class': 'gs_ri'}).find('div', {'class': 'gs_a'})\n",
    "            date = int(author_data.text.split(\"-\")[-2].split(',')[-1].strip())\n",
    "            journal = author_data.text.split(\"-\")[-1]\n",
    "\n",
    "            authors_list = author_data.text.split(\"-\")[0][:-1].split(',')\n",
    "\n",
    "            if len(authors_list) > 3:\n",
    "                authors = authors_list[0] + \" et al.\"\n",
    "            else:\n",
    "                authors = \" &\".join(authors_list)\n",
    "\n",
    "            new_row = pd.DataFrame(columns=['name', 'year', 'authors', 'journal', 'categories', 'key ideas', 'link'], index=[0])\n",
    "\n",
    "            if date >= min_year:\n",
    "                new_df.iloc[i] = [title, date, authors, journal, query, \"\", link] \n",
    "            \n",
    "            #new_df = pd.concat(new_df, new_row)\n",
    "        except:\n",
    "            continue\n",
    "    new_df = new_df.dropna()\n",
    "\n",
    "    df = pd.concat([df, new_df])\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "\n",
    "def extract_clean_text(url, max_chars=5000):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):\n",
    "        tag.decompose()\n",
    "    \n",
    "    main_content = (\n",
    "        soup.find('main') or \n",
    "        soup.find('article') or \n",
    "        soup.find('div', class_=re.compile(r'content|main|article')) or\n",
    "        soup.find('body')\n",
    "    )\n",
    "    \n",
    "    text = main_content.get_text(separator=' ', strip=True) if main_content else soup.get_text(separator=' ', strip=True)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text[:max_chars]\n",
    "\n",
    "def score_relevance(content, description, model=\"llama3.1:8b\"):\n",
    "    prompt = f\"\"\"You are a relevance scoring system. Your ONLY job is to output a single number from 0-100.\n",
    "\n",
    "Description of what we're looking for:\n",
    "{description}\n",
    "\n",
    "Content to evaluate:\n",
    "{content}\n",
    "\n",
    "Instructions:\n",
    "- Score from 0 (completely irrelevant) to 100 (perfectly relevant)\n",
    "- Output ONLY the number, nothing else\n",
    "- No explanation, no text, just the score\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "    response = ollama.generate(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        options={\n",
    "            'temperature': 0.1,  \n",
    "            'num_predict': 10,  \n",
    "            'stop': ['\\n', '.', ' '],  \n",
    "        }\n",
    "    )\n",
    "    \n",
    "    score_text = response['response'].strip()\n",
    "    \n",
    "    try:\n",
    "        score = int(re.search(r'\\d+', score_text).group())\n",
    "        return min(max(score, 0), 100) \n",
    "    except:\n",
    "        print(f\"Warning: Could not parse score from '{score_text}', defaulting to 50\")\n",
    "        return 50\n",
    "    \n",
    "\n",
    "scores = [-1 for i in range(len(df.index))]\n",
    "\n",
    "for i, link in enumerate(df.Hyperlink):\n",
    "    content = extract_clean_text(link)\n",
    "    score = score_relevance(content, description)\n",
    "    scores[i] = score\n",
    "\n",
    "df.insert(1, 'Relevance', scores)\n",
    "df.to_csv('results.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
